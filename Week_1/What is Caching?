Caching is the process of storing frequently used data in a fast storage layer so that future requests can be served quickly without going to the main database or service every time.

Makes your system faster, reduces load, and improves user experience.



Why do we need it?

Imagine Instagram feed:

1 user opens their feed â†’ server fetches posts from the database. âœ…

1 million users open feed â†’ DB crashes! âŒ

Solution: Cache the feed in fast memory (like Redis) so most requests donâ€™t hit the database.

ğŸ”¹ Types of Caching

Client-side caching

Browser stores data temporarily.

Example: Images or JS files in your browser cache.

Server-side caching

Server stores frequent data in memory.

Example: Redis, Memcached storing feed, session data.

CDN caching

Caches static content like images, videos, CSS at edge locations close to users.

Example: Cloudflare, AWS CloudFront

ğŸ”¹ Caching Strategies

Write-Through â€“ Every write goes to DB and cache simultaneously. âœ… Consistent but slower.

Write-Back (Lazy Write) â€“ Write goes to cache first, DB updated later. âœ… Faster but risk of data loss if cache fails.

Cache-Aside (Lazy Loading) â€“ DB is primary; cache is only loaded when needed. âœ… Common in web apps.

ğŸ”¹ Real-World Analogy

Think of library books:

DB = main library archive (slow to access).

Cache = desk with popular books (fast access).

Most students pick books from the desk; only rare ones go to archive.
